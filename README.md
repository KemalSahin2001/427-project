# Customer Support Chatbot with Retrieval-Augmented Generation (RAG)

This project implements an AI-powered customer support assistant using a **Retrieval-Augmented Generation (RAG)** architecture.  
By combining **semantic search** with **advanced generative models**, this system can accurately and efficiently automate customer support queries.  

The solution is designed for scalability â€” starting with the **aviation industry**, but adaptable to other domains and use cases.

---

## ğŸ“Œ Project Objective

- Automate customer support to reduce human workload and operational costs.
- Improve accuracy, relevance, and response time for customer queries.
- Enable scalable multi-domain deployment (starting from aviation industry).
- Build an advanced RAG pipeline with modular components and evaluation tools.

---

## ğŸš€ Key Features

âœ… End-to-end **RAG pipeline** for customer support  
âœ… **Intent extraction**: products, services, issue types, relationships  
âœ… **Vector search** with Elasticsearch  
âœ… Advanced **reranking** of retrieved results  
âœ… Streamlit-based **demo UI**  
âœ… Modular, well-documented Python codebase  
âœ… **RAGAS**-based evaluation pipeline  

---

## ğŸ—‚ï¸ Project Structure

```
MAIN
â”œâ”€â”€ data/                       # Datasets (raw, processed, embeddings, results)
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ test_results/
â”œâ”€â”€ Notebooks/                  # Jupyter notebooks (experiments, tryouts)
â”‚   â”œâ”€â”€ Gpt Pipeline/
â”‚   â”œâ”€â”€ Llama Tryouts/
â”œâ”€â”€ py_files/                   # Core pipeline and helper scripts
â”‚   â”œâ”€â”€ eval/                   # Evaluation pipeline (RAGAS)
â”‚   â”œâ”€â”€ llm_pipeline/           # LLM extraction pipeline
â”‚   â”œâ”€â”€ VectorDBStructure/      # Vector DB helpers
â”‚   â”œâ”€â”€ AIAsistantPipeline.py   # Main RAG pipeline (production)
â”‚   â”œâ”€â”€ QA_Pipeline.py          # QA pipeline
â”‚   â”œâ”€â”€ CONFIG.py               # Config file (paths, prompts)
â”œâ”€â”€ Reports/                    # Project reports and PDFs
â”œâ”€â”€ streamlit_demo.py           # Streamlit app (demo UI)
â”œâ”€â”€ examplepipeline.ipynb       # Example pipeline run
â”œâ”€â”€ ragas_pipeline.ipynb        # RAGAS evaluation notebook
â”œâ”€â”€ README.md                   # Project README (this file)
â”œâ”€â”€ .env                        # Env variables (API keys, Elastic settings)
â”œâ”€â”€ LICENSE
```

---

## ğŸ› ï¸ Solution Workflow

### 1ï¸âƒ£ Data Collection & Preprocessing

- Data source: [Customer Support on Twitter dataset](https://www.kaggle.com/datasets/thoughtvector/customer-support-on-twitter)
- Preprocessing steps:
    - Clean usernames, URLs, special characters
    - Normalize text
    - Tokenize, lemmatize
    - Structure into JSON format with:
        - `chat_id`, `company_name`, `conversation_history`, `entities`, `relationships`, `embedding`

---

### 2ï¸âƒ£ Intent Extraction Pipeline

**Sequential extraction:**

1. **Product extraction**
2. **Service extraction**
3. **Issue type extraction**
4. **Relationship extraction**

â†’ Stored in structured format for downstream embedding & search.

---

### 3ï¸âƒ£ Embedding & Vector Storage

- **Embedding model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Embedding generation**: 
    - Intent embedding
    - Full conversation embedding
- **Vector DB**: Elasticsearch

Search & storage:
- Embeddings pushed to Elastic index
- Cosine similarity for fast retrieval
- Enables "semantic search" of past conversations

---

### 4ï¸âƒ£ Query Processing Pipeline

1. **Preprocessing & vectorization**
2. **Elastic Top-50 retrieval**
3. **Reranking** using cross-encoder (query + candidates)
4. **Top-5 selection**
5. **Response generation** with GPT-4o mini

---

### 5ï¸âƒ£ Evaluation Pipeline

- Custom **RAGAS** score (Retrieval-Augmented Generation Accuracy Score)
- Combines:
    - Retrieval accuracy
    - Generation quality
    - Context alignment

**Result:**  
â†’ Achieved **RAGAS score of 74** on aviation dataset (100 queries).

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Setup environment

```bash
# Clone repo
git clone <your-repo-url>
cd <your-repo-folder>

# Create virtualenv
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Configure .env
cp .env.example .env
# Add your OpenAI API key + Elasticsearch config
```

---

### 2ï¸âƒ£ Run pipeline

```bash
# Run full pipeline
python py_files/AIAsistantPipeline.py
```

---

### 3ï¸âƒ£ Run Streamlit demo UI

```bash
streamlit run streamlit_demo.py
```

---

## ğŸ“‹ Example Usage

```python
from AIAsistantPipeline import ChatQAPipeline

pipeline = ChatQAPipeline()

response, rag_payload = pipeline.run_with_payload(
    "My Echo keeps playing the same song over and over, how do I fix recommendations?"
)

print("LLM Response:", response)
print("RAG Payload:", rag_payload)
```

---

## âš™ï¸ Dependencies

- Python 3.9+
- openai
- sentence-transformers
- elasticsearch
- streamlit
- pandas
- numpy
- tqdm
- python-dotenv

---

## ğŸ“Š Results

- Dataset: 100 test queries (aviation sector)
- RAGAS score: **74**
- High relevance and accuracy in generated responses
- Fast & scalable for real-time customer support

---

## ğŸ§­ Roadmap

âœ… Expand to new industries  
âœ… Add voice command support  
âœ… Optimize reranker for speed  
âœ… Productionize inference pipeline  

---

## ğŸ‘¥ Contributors

- Kemal Åahin 
- Burak Kurt  

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

