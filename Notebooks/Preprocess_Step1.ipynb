{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (replace 'your_dataset.csv' with your file path)\n",
    "data = pd.read_csv('..\\\\Data\\\\raw\\\\twcs\\\\twcs.csv')\n",
    "\n",
    "# Select first 100 unique users who are customers (inbound = True)\n",
    "unique_users = data[data[\"inbound\"] == True][\"author_id\"].unique()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer: Any help here @AdobeCare? https://t.co/x50e57UG4E\n",
      "Company: @115767 Hi Jason, please let us know if there is anything we can do to help. Thanks! ^AJ https://t.co/iuwZCjz4Or\n",
      "Customer: @AdobeCare Pretty much explained my issue in the quoted tweet... dragging an image onto a canvas no longer center snaps it...\n",
      "Company: @115767 Hi Jason, could you please try resetting the preferences of Photoshop &amp; let us know if it helps with the issue https://t.co/j3Dj2HDknM   ^SC\n",
      "Customer: @AdobeCare the ctrl+alt+shift on load method didn't work. (2/2)\n",
      "Company: @115767 Sorry that this isn't more simple... let us know if the steps in the video work out or if you need further help ^Madison\n",
      "Customer: @AdobeCare Didn't work. Tried it on another computer, fresh install, same thing...\n",
      "Company: @115767 Would you please DM the Adobe Product &amp; your purchase details that you have so that we can ask our experts to follow up. ^Raj https://t.co/iuwZCjz4Or\n",
      "Customer: @AdobeCare sure thing\n",
      "Customer: @AdobeCare Will have to try that when I have time to watch a video, for stuff like resetting preferences you shouldn't need a video... (1/2)\n",
      "Company: @115767 I am looping in our expert team to help answer your question. Thanks! ^AJ\n",
      "Customer: @AdobeCare Thanks, it's pretty minor but seriously messes up the workflow I'm used to ;)\n",
      "\n",
      "\n",
      "Customer: Hey @XboxSupport is there issues with GS being reported at the moment? Noticed a few games I have 100% are reporting 0/1000...\n",
      "Company: @115767 and redownload your profile and see if the achievements show up again?  2 ^JS\n",
      "\n",
      "\n",
      "Customer: Hey @XboxSupport is there issues with GS being reported at the moment? Noticed a few games I have 100% are reporting 0/1000...\n",
      "Company: @115767 Hi there! Could you remove your profile https://t.co/zJxn1EMf83 , power cycle https://t.co/QENEhDawC3  1 ^JS\n",
      "Customer: @XboxSupport Will try that when I get home - going to DM for a longer response...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocess the data\n",
    "data[\"in_response_to_tweet_id\"] = data[\"in_response_to_tweet_id\"].fillna(-1).astype(int)\n",
    "data[\"response_tweet_id\"] = data[\"response_tweet_id\"].fillna(\"-1\")\n",
    "\n",
    "# Helper function to extract responses\n",
    "def extract_responses(response_ids):\n",
    "    try:\n",
    "        return [int(response_ids)]\n",
    "    except:\n",
    "        return [int(x) for x in response_ids.split(\",\")]\n",
    "\n",
    "# Recursive function to extract the conversation\n",
    "def extract_conversation(conv, response_num, data, comp_name):\n",
    "    if response_num == -1:\n",
    "        # End of the conversation\n",
    "        return conv, comp_name\n",
    "    else:\n",
    "        row = data[data[\"tweet_id\"] == response_num]\n",
    "        if len(row) == 0:\n",
    "            # End of the conversation\n",
    "            return conv, comp_name\n",
    "\n",
    "        conv += \"\\n\"\n",
    "        inbound_val = row[\"inbound\"].values[0]\n",
    "        if inbound_val:\n",
    "            conv += \"Customer: \"\n",
    "        else:\n",
    "            conv += \"Company: \"\n",
    "            if comp_name is None:\n",
    "                comp_name = row[\"author_id\"].values[0]\n",
    "        conv += row[\"text\"].values[0]\n",
    "\n",
    "        # Process responses recursively\n",
    "        responses = extract_responses(row[\"response_tweet_id\"].values[0])\n",
    "        for i in responses:\n",
    "            conv, comp_name = extract_conversation(conv, i, data, comp_name)\n",
    "        return conv, comp_name\n",
    "\n",
    "# Initialize lists for storing results\n",
    "all_conversations = []\n",
    "all_company_names = []\n",
    "user_based_convs = []\n",
    "\n",
    "# Extract user conversations\n",
    "for userid in tqdm(unique_users):\n",
    "    user_conversations = []\n",
    "    user_requests = data[(data[\"author_id\"] == userid) & (data[\"in_response_to_tweet_id\"] == -1)]\n",
    "    for i in range(len(user_requests)):\n",
    "        conv = \"Customer: \" + user_requests.iloc[i][\"text\"]\n",
    "        responses = extract_responses(user_requests.iloc[i][\"response_tweet_id\"])\n",
    "        for response in responses:\n",
    "            convers, comp_name = extract_conversation(conv, response, data, comp_name=None)\n",
    "            all_conversations.append(convers)\n",
    "            all_company_names.append(comp_name)\n",
    "            user_conversations.append(convers)\n",
    "    user_based_convs.append(user_conversations)\n",
    "\n",
    "# Map user IDs to conversations\n",
    "user_id_list = []\n",
    "for i in range(len(user_based_convs)):\n",
    "    user_id_list += [unique_users[i]] * len(user_based_convs[i])\n",
    "\n",
    "# Create a DataFrame for conversations\n",
    "df = pd.DataFrame({\"user_id\": user_id_list, \"conversations\": all_conversations, \"company_name\": all_company_names})\n",
    "\n",
    "# Function to print conversation history for a specific user\n",
    "def print_conv_History(user_id):\n",
    "    conversations = df[df[\"user_id\"] == user_id][\"conversations\"].values\n",
    "    for i in conversations:\n",
    "        print(i)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Example: Print conversation history for a specific user ID\n",
    "print_conv_History(\"115767\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"..\\\\Data/processed_conversations_10k.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: find all subset conversations that are present in the parent conversation and drop them\n",
    "def find_subsets(df):\n",
    "    subset_records = []\n",
    "    grouped = df.groupby(['user_id', 'company_name'])\n",
    "    \n",
    "    for (user_id, company_name), group in grouped:\n",
    "        conversations = group['conversations'].tolist()\n",
    "        for i, convo1 in enumerate(conversations):\n",
    "            for j, convo2 in enumerate(conversations):\n",
    "                if i != j and convo1 in convo2:\n",
    "                    subset_records.append((user_id, company_name, convo1, convo2))\n",
    "    \n",
    "    return pd.DataFrame(subset_records, columns=['user_id', 'company_name', 'subset_conversation', 'parent_conversation'])\n",
    "\n",
    "# Identify subset conversations\n",
    "subset_df = find_subsets(sample_data)\n",
    "\n",
    "# Function to drop subset conversations\n",
    "def drop_subset_conversations(df):\n",
    "    subset_df = find_subsets(df)\n",
    "    subset_conversations = subset_df['subset_conversation'].unique()\n",
    "    cleaned_df = df[~df['conversations'].isin(subset_conversations)]\n",
    "    return cleaned_df\n",
    "\n",
    "# Drop subset conversations from the original dataset\n",
    "sample_data = drop_subset_conversations(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Correct spelling\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s\\n]', '', text)  # Remove special characters but keep newline characters\n",
    "    text = re.sub(r'[ \\t]+', ' ', text).strip()  # Remove extra spaces but preserve newlines\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the conversations column\n",
    "sample_data['cleaned_conversations'] = sample_data['conversations'].apply(clean_text)\n",
    "\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to correct misspelled words\n",
    "def correct_mistypo(text):\n",
    "    corrected_text = []\n",
    "    for word in text.split():\n",
    "        corrected_word = spell.correction(word)  # Suggest the most likely correction\n",
    "        corrected_text.append(corrected_word if corrected_word else word)\n",
    "    return ' '.join(corrected_text)\n",
    "\n",
    "# Apply the correction function to the cleaned conversations\n",
    "sample_data['corrected_conversations'] = sample_data['cleaned_conversations'].apply(correct_mistypo)\n",
    "\n",
    "sample_data.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
