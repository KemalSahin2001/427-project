{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96109c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_files.llm_pipeline.twcs_processor as processor\n",
    "import pandas as pd\n",
    "import json\n",
    "from py_files.VectorDBStructure.query import query_similar\n",
    "from py_files.llm_pipeline.reranker import CrossEncoderReranker\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Dict\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "from CONFIG import ENDBOT_PROMPT\n",
    "\n",
    "# Load environment variables from .env file\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set OpenAI API key\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# User input part\n",
    "prompts = pd.read_excel(\"data/processed/test_data/airway_test_data.xlsx\")\n",
    "results = []\n",
    "count = 0\n",
    "\n",
    "# Initialize the models once\n",
    "from py_files.VectorDBStructure.db_structure import DatabaseStructure\n",
    "db = DatabaseStructure()\n",
    "reranker = CrossEncoderReranker(top_k=50)\n",
    "\n",
    "for user_input in prompts['Conversation']:\n",
    "    count += 1\n",
    "    user_input = processor.TWCSProcessor._clean_single(user_input)\n",
    "    user_input_processed = processor.TWCSProcessor._convert_to_conversation(user_input)\n",
    "\n",
    "    # convert the user input to a pandas dataframe\n",
    "    user_input_df = pd.DataFrame([[user_input,user_input_processed]], columns=['cleaned_conversation','structured_conversations'])\n",
    "\n",
    "    from py_files.llm_pipeline.llm_extractor import LLMExtractor\n",
    "    pipe = LLMExtractor(dataframe = user_input_df)\n",
    "\n",
    "    # only products / issue-types / services\n",
    "    df1 = pipe.extract_entities()\n",
    "\n",
    "    # pack them into a single JSON field\n",
    "    df2 = pipe.process_entities_json()\n",
    "\n",
    "    # create RDF triples\n",
    "    df3 = pipe.extract_relationships()\n",
    "\n",
    "    # init db carried out\n",
    "\n",
    "    cleaned_conversations = user_input_processed\n",
    "    entities = df3['entities'].values[0]\n",
    "    relationship = df3['relationship'].values[0]\n",
    "    cleaned_conversation = df3['cleaned_conversation'].values[0]\n",
    "\n",
    "    fixed_relationships = db.fix_relationships(relationship)\n",
    "\n",
    "    embedding = db.text_to_embedding(cleaned_conversation, entities, fixed_relationships).tolist()\n",
    "\n",
    "    # Initialize the reranker carried out\n",
    "    \n",
    "    # Step 2: Query Elastic and get hits\n",
    "    hits = query_similar(embedding, k=50)\n",
    "\n",
    "    # Step 3: Extract conversation candidates\n",
    "    candidates = [hit[\"_source\"][\"Conversation_History\"][\"conversation\"] for hit in hits]\n",
    "\n",
    "    # Step 4: Rerank with cross-encoder\n",
    "    reranked = reranker.rerank(user_input, candidates)\n",
    "\n",
    "    # Step 5: Create a mapping: conversation -> (score, rank)\n",
    "    score_rank_map = {\n",
    "        conv: (score, rank + 1)  # rank is 1-based\n",
    "        for rank, (conv, score) in enumerate(reranked)\n",
    "    }\n",
    "\n",
    "    # Step 6: Construct final rows with rank\n",
    "    rows = []\n",
    "    for hit in hits:\n",
    "        src = hit[\"_source\"]\n",
    "        conv = src[\"Conversation_History\"][\"conversation\"]\n",
    "        score, rank = score_rank_map.get(conv, (0.0, None))  # Not reranked if not in top_k\n",
    "\n",
    "        rows.append({\n",
    "            \"prompt\": user_input,\n",
    "            \"id\": hit[\"_id\"],\n",
    "            \"similarity_score\": hit[\"_score\"],\n",
    "            \"rerank_score\": score,\n",
    "            \"rerank_rank\": rank,\n",
    "            \"ChatID\": src[\"ChatID\"],\n",
    "            \"Company_name\": src[\"Company_name\"],\n",
    "            \"Conversation_History\": conv,\n",
    "            \"Entities\": json.dumps(src[\"Entities\"]),\n",
    "            \"Relationships\": json.dumps(src[\"Relationships\"])\n",
    "        })\n",
    "\n",
    "    # Step 7: Create DataFrame and optionally sort by rerank_rank\n",
    "    reranked_qa = pd.DataFrame(rows).sort_values(by=\"rerank_rank\", na_position=\"last\").reset_index(drop=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    reranked_qa[[\"sim_norm\", \"rerank_norm\"]] = scaler.fit_transform(\n",
    "        reranked_qa[[\"similarity_score\", \"rerank_score\"]].fillna(0)\n",
    "    )\n",
    "    reranked_qa[\"hybrid_score\"] = 0.7 * reranked_qa[\"sim_norm\"] + 0.3 * reranked_qa[\"rerank_norm\"]\n",
    "\n",
    "    top_candidates = reranked_qa.sort_values(by=\"hybrid_score\", ascending=False).head(10)\n",
    "\n",
    "    seen_combinations = set()\n",
    "    filtered_rows = []\n",
    "\n",
    "    for _, row in top_candidates.iterrows():\n",
    "        key = (row[\"Entities\"], row[\"Relationships\"])\n",
    "        if key not in seen_combinations:\n",
    "            filtered_rows.append(row)\n",
    "            seen_combinations.add(key)\n",
    "        if len(filtered_rows) == 5:\n",
    "            break\n",
    "\n",
    "    top5_df = pd.DataFrame(filtered_rows)\n",
    "\n",
    "    def parse_conversation(text: str) -> List[Dict[str, str]]:\n",
    "        lines = text.split(\"\\n\")\n",
    "        parsed = []\n",
    "        for line in lines:\n",
    "            lower = line.lower()\n",
    "            if lower.startswith(\"customer\"):\n",
    "                role = \"Customer\"\n",
    "                msg = line[len(\"Customer\"):].strip()\n",
    "            elif lower.startswith(\"company\"):\n",
    "                role = \"Company\"\n",
    "                msg = line[len(\"Company\"):].strip()\n",
    "            else:\n",
    "                # fallback: use last role or unknown\n",
    "                role = \"Customer\" if not parsed else parsed[-1][\"role\"]\n",
    "                msg = line.strip()\n",
    "            if msg:\n",
    "                parsed.append({\"role\": role, \"message\": msg})\n",
    "        return parsed\n",
    "\n",
    "    def build_payload_per_qa(df_top5, query: str) -> str:\n",
    "        results = []\n",
    "        for _, row in df_top5.iterrows():\n",
    "            conv = row[\"Conversation_History\"]\n",
    "\n",
    "            if isinstance(conv, str):\n",
    "                try:\n",
    "                    conv_json = json.loads(conv)\n",
    "                    conversation = conv_json  # Already parsed\n",
    "                except:\n",
    "                    conversation = parse_conversation(conv)\n",
    "            else:\n",
    "                conversation = conv\n",
    "\n",
    "            try:\n",
    "                intents = json.loads(row[\"Entities\"])\n",
    "            except:\n",
    "                intents = {}\n",
    "\n",
    "            try:\n",
    "                relationships = json.loads(row[\"Relationships\"])\n",
    "            except:\n",
    "                relationships = []\n",
    "\n",
    "            results.append({\n",
    "                \"company_name\": row[\"Company_name\"],\n",
    "                \"conversation\": conversation,\n",
    "                \"intents\": intents,\n",
    "                \"relationships\": relationships\n",
    "            })\n",
    "\n",
    "        full_payload = {\n",
    "            \"query\": query.strip(),\n",
    "            \"retrieved_answers\": results\n",
    "        }\n",
    "\n",
    "        return json.dumps(full_payload, ensure_ascii=False, indent=2), results\n",
    "\n",
    "    payload, retrievals = build_payload_per_qa(\n",
    "        df_top5=top5_df.sort_values(by=\"hybrid_score\", ascending=False).head(5),\n",
    "        query=user_input\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": ENDBOT_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": payload}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    # Response generated\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    results.append({\n",
    "        \"prompts\" : user_input,\n",
    "        \"retrievals\" : retrievals,\n",
    "        \"answers\" : answer\n",
    "    })\n",
    "    print(f\"Row {count} completed.\")\n",
    "\n",
    "result = pd.DataFrame(results)\n",
    "result.to_excel('results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outlier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
